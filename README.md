# Data-Analyst-Jobs-in-SA
Overview:
This project involves the use of web scraping, SQL, and Tableau to create an interactive visualization of junior data analyst jobs in South Africa. The data used in this project is obtained by scraping job postings from the Indeed website using Selenium and BeautifulSoup. The scraped data is then cleaned and processed using SQL, and exported to an Excel file. The Excel file is then imported into Tableau for visualization.

Requirements:
To run this project, you will need to have the following software and libraries installed:

Python 3
Selenium
BeautifulSoup
Pandas
SQLAlchemy
Tableau Desktop
Installation:
To install Python 3, you can download it from the official Python website: https://www.python.org/downloads/

To install the required libraries, you can use pip, a package installer for Python. Run the following commands in your terminal or command prompt:
pip install selenium
pip install beautifulsoup4
pip install pandas
pip install sqlalchemy

To install Tableau Desktop, you can download it from the official Tableau website: https://www.tableau.com/products/desktop/download

Data:
The data used in this project is obtained by scraping job postings from the Indeed website. The job postings should be saved in a CSV file format and saved in a directory within the project folder. The file name should be provided in the code where the data is being read.

Project Structure:

The main Python code file is named 'indeedwebscrapping.py'.
The project folder also includes a 'data' folder where the scraped data is saved.
The 'sql' folder contains the SQL code used to clean and process the data.
The 'output' folder contains the output generated by the code, such as the cleaned data and the Excel file.
The 'tableau' folder contains the Tableau workbook and associated files.
Execution:
To run the project, follow these steps:

Run the 'scraper.py' file in a Python environment. This will scrape the job postings from the Indeed website and save them to a CSV file in the 'data' folder.
Open the SQL file in a SQL environment and run the code to clean and process the data. This will generate a cleaned data file in the 'output' folder.
Import the cleaned data file into Tableau and use it to create visualizations.
Functionality:

Conclusion:
This project demonstrates how to use web scraping, SQL, and Tableau to create an interactive visualization of junior data analyst jobs in South Africa. It can be further extended to include more advanced techniques such as text analysis of job descriptions, sentiment analysis of job postings, and predictive modeling of job demand.
